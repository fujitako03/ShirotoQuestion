{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前準備\n",
    "## ライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def classify_source(url):\n",
    "    \"\"\"\n",
    "    分類リクエストを送信する\n",
    "\n",
    "    Args:\n",
    "        message (str): メッセージ\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    if parsed_url.netloc == \"brainpad.atlassian.net\":\n",
    "        return \"confluence\"\n",
    "    elif parsed_url.netloc == \"blog.brainpad.co.jp\":\n",
    "        return \"platinum_data_blog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python.exe -m pip install --upgrade pip\n",
    "!pip install atlassian-python-api\n",
    "!pip install langchain\n",
    "!pip install typing_extensions==4.5.0\n",
    "!pip install typing-inspect==0.8.0\n",
    "!pip install beautifulsoup4\n",
    "!pip install lxml\n",
    "!pip install pytesseract Pillow\n",
    "!pip install openai\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from atlassian import Confluence\n",
    "\n",
    "from langchain.document_loaders import ConfluenceLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "from openai import ChatCompletion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 設定クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conf:\n",
    "    def __init__(self, user_name, confluence_api_key, openai_api_key):\n",
    "        self.user_name = user_name\n",
    "        self.confluence_api_key = confluence_api_key\n",
    "        self.openai_api_key = openai_api_key\n",
    "conf = Conf(\n",
    "    user_name=\"\",\n",
    "    confluence_api_key=\"\",\n",
    "    openai_api_key=\"\",\n",
    ")\n",
    "# CONFLUENCE_USERNAME\n",
    "# CONFLUENCE_API_KEY\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実装"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confluence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfluencePage:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "    @property\n",
    "    def base_url(self):\n",
    "        parsed_url = urlparse(self.url)\n",
    "        return parsed_url.scheme + \"://\" + parsed_url.netloc\n",
    "\n",
    "    @property\n",
    "    def space_id(self):\n",
    "        return self.url.split('/')[5]\n",
    "    \n",
    "    @property\n",
    "    def page_id(self):\n",
    "        return self.url.split('/')[7]\n",
    "    \n",
    "    def get_documents(self, user_name, api_key):\n",
    "        loader = ConfluenceLoader(\n",
    "            url=self.base_url,\n",
    "            username=user_name,\n",
    "            api_key=api_key\n",
    "        )\n",
    "        documents = loader.load(\n",
    "            page_ids=[self.page_id],\n",
    "            include_attachments=False,\n",
    "            limit=10)\n",
    "        return documents\n",
    "\n",
    "    def get_html(self, user_name, api_key):\n",
    "        confluence = Confluence(\n",
    "            url=self.base_url,\n",
    "            username=user_name,\n",
    "            password=api_key\n",
    "        )\n",
    "        page_info = confluence.get_page_by_id(page_id=self.page_id, expand='body.storage')\n",
    "        \n",
    "        return page_info[\"body\"][\"storage\"][\"value\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentPreprocessor:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def create_document(self, html,  url, chunk_size=1000):\n",
    "        text = self.html_to_text(html)\n",
    "        docs = self.text_to_documents(text=text, url=url, chunk_size=chunk_size)\n",
    "        return docs\n",
    "\n",
    "    def html_to_text(self, html):\n",
    "        \"\"\"\n",
    "        HTMLをテキストに変換します\n",
    "        \"\"\"\n",
    "        soup=BeautifulSoup(html,\"html.parser\")\n",
    "        text=soup.get_text('\\n')\n",
    "        lines= [line.strip() for line in text.splitlines()]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def text_to_documents(self, text, url, chunk_size):\n",
    "        \"\"\"\n",
    "        ドキュメントをいくつかのチャンクに分割します。\n",
    "        \"\"\"\n",
    "        text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=chunk_size, chunk_overlap=0)\n",
    "        docs = text_splitter.create_documents([text], metadatas=[{\"source\": url}])\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionGenerator:\n",
    "    def __init__(self, llm):\n",
    "        self.conf = conf\n",
    "        self.llm = llm\n",
    "\n",
    "    def get_topics(self, docs):\n",
    "        # トピックの抽出\n",
    "        query = \"\"\"\n",
    "        この文章のトピックを一言で表すと何と言えますか\n",
    "        \"\"\"\n",
    "        chain = load_qa_with_sources_chain(self.llm, chain_type=\"map_reduce\")\n",
    "        topic_text = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\n",
    "        return topic_text\n",
    "    \n",
    "    def generate_topic_issues(self, topic):\n",
    "        # 論点の抽出\n",
    "        template = \"\"\"\n",
    "        以下のトピックについて議論をする際に重要な論点を網羅的に羅列してください。\n",
    "        {topic_text}\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"topic_text\"]\n",
    "        )\n",
    "        prompt_text = prompt.format(topic_text=[topic])\n",
    "\n",
    "        # \n",
    "        response = ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        #ChatGPTの回答を出力\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    def find_missing_issues(self, issues, docs):\n",
    "        template = \"\"\"\n",
    "        以下の重要な論点のうち、ここまでの議論で取り上げられていないものはどれですか\n",
    "        {issue_text}\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"issue_text\"]\n",
    "        )\n",
    "        prompt_text = prompt.format(issue_text=issues)\n",
    "        print(prompt_text)\n",
    "\n",
    "        chain = load_qa_with_sources_chain(self.llm, chain_type=\"map_reduce\")\n",
    "        question_points = chain({\"input_documents\": docs, \"question\": prompt_text}, return_only_outputs=True)\n",
    "        return question_points\n",
    "    \n",
    "    def generate_question(self, missing_issues):\n",
    "        # プロンプトの生成\n",
    "        template = \"\"\"\n",
    "        あなたは親切なアシスタントです。質問したい内容についてより深く知るための質問を5つ生成してください。\n",
    "        質問は全て「大変興味深い発表ありがとうございます。素人質問で恐縮ですが、」に続く形で簡潔にしてください。\n",
    "\n",
    "        # 質問したい内容\n",
    "        {missing_issues}\n",
    "\n",
    "        # フォーマット\n",
    "        1. [question1]\n",
    "        2. [question2]\n",
    "        3. [question3]\n",
    "        4. [question4]\n",
    "        5. [question5]\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"missing_issues\"]\n",
    "        )\n",
    "        prompt_text = prompt.format(\n",
    "            missing_issues=missing_issues)\n",
    "\n",
    "        # 質問の生成\n",
    "        response = ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        #ChatGPTの回答を出力\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実行"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://****\" # コンペ事業\n",
    "# conflueceデータの読み込み\n",
    "confulence_page = ConfluencePage(\n",
    "    url=url,\n",
    ")\n",
    "html = confulence_page.get_html(\n",
    "    user_name=conf.user_name,\n",
    "    api_key=conf.confluence_api_key,\n",
    "    )\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_prep = DocumentPreprocessor()\n",
    "docs = document_prep.create_document(html, url)\n",
    "docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 質問の生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "llm = ChatOpenAI(openai_api_key=conf.openai_api_key, model_name= \"gpt-3.5-turbo\", temperature=0.3)\n",
    "qgen = QuestionGenerator(llm)\n",
    "topic = qgen.get_topics(docs)\n",
    "issues = qgen.generate_topic_issues(topic[\"output_text\"])\n",
    "missing_issues = qgen.find_missing_issues(issues, docs)\n",
    "question = qgen.generate_question(missing_issues)\n",
    "print(\"[topic]\\n\", topic)\n",
    "print(\"[issues]\\n\", issues)\n",
    "print(\"[missing_issues]\\n\", missing_issues[\"output_text\"])\n",
    "print(\"[question]\\n\", question)\n",
    "question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP1 内容を要約する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1 質問すべきポイントを指摘する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"\n",
    "以下の重要な論点のうち、ここまでの議論で取り上げられていないものはどれですか\n",
    "{issue_text}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"issue_text\"]\n",
    ")\n",
    "prompt_text = prompt.format(issue_text=[response[\"choices\"][0][\"message\"][\"content\"]])\n",
    "print(prompt_text)\n",
    "\n",
    "chain = load_qa_with_sources_chain(llm, chain_type=\"map_reduce\")\n",
    "question_points = chain({\"input_documents\": docs, \"question\": prompt_text}, return_only_outputs=True)\n",
    "question_points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ブレインパッドの新規事業の内容を具体的に教えて。 output:ja\n",
    "    - {'output_text': ' BrainPadの新規事業の内容は、三者間の協力によってメリットを得られること、プレゼンスを向上させること、競合他社との差別化を図ること、優秀な参加者への唾つけをすること、案件外組織との関係を獲得すること、受託分析に流せる可能性を持つこと、AS部の既存課題を改善するシナジーを生むことなどがあります。\\nSOURCES: https://brainpad.atlassian.net/wiki/spaces/~yuta.yoshida/pages/3595404469/AS'}\n",
    "- 資料の内容を踏まえて感想文を180文字程度で書いて\n",
    "    - {'output_text': ' ブレインパッド社が開催するデータ分析コンペ「白金鉱業Cup #1」は、受託分析とは異なるプランでありながら、金銭価値だけでなくリード獲得やコネクション獲得、採用貢献などの多くの効果を期待できると私は考えます。参加企業をはじめ大学・行政・toC企業・ベンチャーなどが関わり、それぞれが持つメリットを生かし、多様な価値を創出していくことが可能であると'}\n",
    "- ここまでの内容を踏まえて、重要な説明が不足しているポイントを2つ教えてください。\n",
    "    - {'output_text': ' The two points that are missing important explanations are 1) the cost (how much people can be allocated) and 2) the opportunities beyond the monetary value from the competition itself, such as lead acquisition, connection acquisition, contribution to recruitment (cost reduction to recruiters), enhancement of brand image, partial improvement of organizational issues, differentiation from other companies, etc.\\nSOURCES: https://brainpad.atlassian.net/wiki/spaces/~yuta.yoshida/pages/3595404469/AS'}\n",
    "- ここまでの内容を踏まえて、重要であるが説明が不足しているポイントは何がありますか？\n",
    "    - Based on the given portion of the document, it is difficult to determine a specific point that is important but lacking in explanation. However, potential areas where more explanation may be needed include the specific goals and objectives of the proposed ML competitions and data hackathons, the potential benefits and drawbacks of the proposed initiatives, the resources and support needed to implement the initiatives, the potential impact on the AS department's reputation and relationships, the potential risks and challenges associated with the initiatives, and the potential role of the AS department in promoting and facilitating the initiatives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2 質問文の形に整形する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "あなたは親切なアシスタントです。質問したい内容についてより深く知るために、ここまでの議論の要約を踏まえて「大変興味深い発表ありがとうございます。素人質問で恐縮ですが、」に続く形で質問をしてください。\n",
    "\n",
    "# 質問したい内容\n",
    "{question_points}\n",
    "\n",
    "# ここまでの議論の要約\n",
    "{summary}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question_points\", \"summary\"]\n",
    ")\n",
    "prompt_text = prompt.format(\n",
    "    question_points=[question_points[\"output_text\"]],\n",
    "    summary=[summary])\n",
    "print(prompt_text)\n",
    "\n",
    "response = ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    ")\n",
    "\n",
    "#ChatGPTの回答を出力\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "あなたは親切なアシスタントです。以下の内容についてもう少し深く知るために「大変興味深い発表ありがとうございます。素人質問で恐縮ですが、」に続く形で質問をしてください。複数のポイントがある場合、重要な一つに絞って質問するようにしてください。\n",
    "[\"Based on the given portion of the document, it is difficult to determine a specific point that is important but lacking in explanation. However, potential areas where more explanation may be needed include the specific goals and objectives of the proposed ML competitions and data hackathons, the potential benefits and drawbacks of the proposed initiatives, the resources and support needed to implement the initiatives, the potential impact on the AS department's reputation and relationships, the potential risks and challenges associated with the initiatives, and the potential role of the AS department in promoting and facilitating the initiatives. \\nSOURCES: https://brainpad.atlassian.net/wiki/spaces/~yuta.yoshida/pages/3595404469/AS\"]\n",
    "\n",
    "大変興味深い発表ありがとうございます。素人質問で恐縮ですが、提案されたMLの競技会やデータハッカソンの具体的な目標や目的、提案されたイニシアチブの潜在的な利点や欠点、イニシアチブを実施するために必要なリソースやサポート、AS部門の評判や関係に与える潜在的な影響、イニシアチブに関連する潜在的なリスクや課題、そしてAS部門のイニシアチブを促進・支援する上での潜在的な役割について、もう少し詳しく教えていただけますか？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
